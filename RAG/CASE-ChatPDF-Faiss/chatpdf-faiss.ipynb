{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6750ec-c305-4004-9a9a-09664aa59813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# 把 .env 里所有 KEY=VALUE 注入到 os.environ\n",
    "load_dotenv()          # 默认查找当前目录下的 .env\n",
    "\n",
    "DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    raise ValueError(\"请设置环境变量 DASHSCOPE_API_KEY\")\n",
    "\n",
    "def extract_text_with_page_numbers(pdf) -> Tuple[str, List[Tuple[str, int]]]:\n",
    "    \"\"\"\n",
    "    从PDF中提取文本并记录每个字符对应的页码\n",
    "    \n",
    "    参数:\n",
    "        pdf: PDF文件对象\n",
    "    \n",
    "    返回:\n",
    "        text: 提取的文本内容\n",
    "        char_page_mapping: 每个字符对应的页码列表\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    char_page_mapping = []\n",
    "\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:\n",
    "            text += extracted_text\n",
    "            # 为当前页面的每个字符记录页码\n",
    "            char_page_mapping.extend([page_number] * len(extracted_text))\n",
    "        else:\n",
    "            print(f\"No text found on page {page_number}.\")\n",
    "\n",
    "    return text, char_page_mapping\n",
    "\n",
    "def process_text_with_splitter(text: str, char_page_mapping: List[int], save_path: str = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    处理文本并创建向量存储\n",
    "    \n",
    "    参数:\n",
    "        text: 提取的文本内容\n",
    "        char_page_mapping: 每个字符对应的页码列表\n",
    "        save_path: 可选，保存向量数据库的路径\n",
    "    \n",
    "    返回:\n",
    "        knowledgeBase: 基于FAISS的向量存储对象\n",
    "    \"\"\"\n",
    "    # 创建文本分割器，用于将长文本分割成小块\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    # 分割文本\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"文本被分割成 {len(chunks)} 个块。\")\n",
    "        \n",
    "    # 创建嵌入模型\n",
    "    embeddings = DashScopeEmbeddings(\n",
    "        model=\"text-embedding-v1\",\n",
    "        dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "    )\n",
    "    \n",
    "    # 从文本块创建知识库\n",
    "    knowledgeBase = FAISS.from_texts(chunks, embeddings)\n",
    "    print(\"已从文本块创建知识库。\")\n",
    "    \n",
    "    # 为每个文本块找到对应的页码信息\n",
    "    page_info = {}\n",
    "    current_pos = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_start = current_pos\n",
    "        chunk_end = current_pos + len(chunk)\n",
    "        \n",
    "        # 找到这个文本块中字符对应的页码\n",
    "        chunk_pages = char_page_mapping[chunk_start:chunk_end]\n",
    "        \n",
    "        # 取页码的众数（出现最多的页码）作为该块的页码\n",
    "        if chunk_pages:\n",
    "            # 统计每个页码出现的次数\n",
    "            page_counts = {}\n",
    "            for page in chunk_pages:\n",
    "                page_counts[page] = page_counts.get(page, 0) + 1\n",
    "            \n",
    "            # 找到出现次数最多的页码\n",
    "            most_common_page = max(page_counts, key=page_counts.get)\n",
    "            page_info[chunk] = most_common_page\n",
    "        else:\n",
    "            page_info[chunk] = 1  # 默认页码\n",
    "        \n",
    "        current_pos = chunk_end\n",
    "    \n",
    "    knowledgeBase.page_info = page_info\n",
    "    print(f'页码映射完成，共 {len(page_info)} 个文本块')\n",
    "    \n",
    "    # 如果提供了保存路径，则保存向量数据库和页码信息\n",
    "    if save_path:\n",
    "        # 确保目录存在\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # 保存FAISS向量数据库\n",
    "        knowledgeBase.save_local(save_path)\n",
    "        print(f\"向量数据库已保存到: {save_path}\")\n",
    "        \n",
    "        # 保存页码信息到同一目录\n",
    "        with open(os.path.join(save_path, \"page_info.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(page_info, f)\n",
    "        print(f\"页码信息已保存到: {os.path.join(save_path, 'page_info.pkl')}\")\n",
    "    \n",
    "    return knowledgeBase\n",
    "\n",
    "def load_knowledge_base(load_path: str, embeddings = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    从磁盘加载向量数据库和页码信息\n",
    "    \n",
    "    参数:\n",
    "        load_path: 向量数据库的保存路径\n",
    "        embeddings: 可选，嵌入模型。如果为None，将创建一个新的DashScopeEmbeddings实例\n",
    "    \n",
    "    返回:\n",
    "        knowledgeBase: 加载的FAISS向量数据库对象\n",
    "    \"\"\"\n",
    "    # 如果没有提供嵌入模型，则创建一个新的\n",
    "    if embeddings is None:\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "        )\n",
    "    \n",
    "    # 加载FAISS向量数据库，添加allow_dangerous_deserialization=True参数以允许反序列化\n",
    "    knowledgeBase = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(f\"向量数据库已从 {load_path} 加载。\")\n",
    "    \n",
    "    # 加载页码信息\n",
    "    page_info_path = os.path.join(load_path, \"page_info.pkl\")\n",
    "    if os.path.exists(page_info_path):\n",
    "        with open(page_info_path, \"rb\") as f:\n",
    "            page_info = pickle.load(f)\n",
    "        knowledgeBase.page_info = page_info\n",
    "        print(\"页码信息已加载。\")\n",
    "    else:\n",
    "        print(\"警告: 未找到页码信息文件。\")\n",
    "    \n",
    "    return knowledgeBase\n",
    "\n",
    "# 读取PDF文件\n",
    "pdf_reader = PdfReader('./浦发上海浦东发展银行西安分行个金客户经理考核办法.pdf')\n",
    "# 提取文本和页码信息\n",
    "text, char_page_mapping = extract_text_with_page_numbers(pdf_reader)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0201b5-0525-4cab-b966-9d31e5540d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"提取的文本长度: {len(text)} 个字符。\")\n",
    "    \n",
    "# 处理文本并创建知识库，同时保存到磁盘\n",
    "save_dir = \"./vector_db\"\n",
    "knowledgeBase = process_text_with_splitter(text, char_page_mapping, save_path=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa66af-cfb3-4779-94c7-4e16f9627b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "llm = Tongyi(model_name=\"deepseek-v3\", dashscope_api_key=DASHSCOPE_API_KEY) # qwen-turbo\n",
    "\n",
    "# 设置查询问题\n",
    "query = \"客户经理被投诉了，投诉一次扣多少分\"\n",
    "#query = \"客户经理每年评聘申报时间是怎样的？\"\n",
    "if query:\n",
    "    # 执行相似度搜索，找到与查询相关的文档\n",
    "    docs = knowledgeBase.similarity_search(query,k=10)\n",
    "\n",
    "    # 构建上下文\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # 构建提示\n",
    "    prompt = f\"\"\"根据以下上下文回答问题:\n",
    "\n",
    "{context}\n",
    "\n",
    "问题: {query}\"\"\"\n",
    "\n",
    "    # 直接调用 LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    print(response)\n",
    "    print(\"来源:\")\n",
    "\n",
    "    # 记录唯一的页码\n",
    "    unique_pages = set()\n",
    "\n",
    "    # 显示每个文档块的来源页码\n",
    "    for doc in docs:\n",
    "        #print('doc=',doc)\n",
    "        text_content = getattr(doc, \"page_content\", \"\")\n",
    "        source_page = knowledgeBase.page_info.get(\n",
    "            text_content.strip(), \"未知\"\n",
    "        )\n",
    "\n",
    "        if source_page not in unique_pages:\n",
    "            unique_pages.add(source_page)\n",
    "            print(f\"文本块页码: {source_page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a95ab-8e95-4a8f-be27-7bb1f350f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
